{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datascience \n",
    "import numpy as np\n",
    "import graphviz\n",
    "\n",
    "from datascience import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "  \n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=Table.read_table('W:\\Climate1_.csv' , sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = Table.read_table('W:\\ClimateBal.csv' , sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = t1.append(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3.to_csv(\"MergedLabelledData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>ID</th> <th>Text</th> <th>Support</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>1075_Cleand_Climate1.csv</td> <td>RT @sydneyleemarco: nothing like an 80 degree october da ...</td> <td>1      </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>85_Cleand_Climate1.csv  </td> <td>@MerlenesMemos @CNN It's not an act of god. Climate chan ...</td> <td>1      </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>916_Cleand_Climate1.csv </td> <td>RT @gq_jayq: Bet I got 11 years to run it up https://t.c ...</td> <td>1      </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>557_Cleand_Climate1.csv </td> <td>RT @HazelMonforton: Literally none of these things will  ...</td> <td>1      </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>39_Cleand_Climate1.csv  </td> <td>@BjornLomborg I think looking at climate change in terms ...</td> <td>1      </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>608_Cleand_Climate1.csv </td> <td>RT @KateRaworth: If you feel the dissonance between the  ...</td> <td>1      </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>121_Cleand_Climate1.csv </td> <td>@Weinsteinlaw His base doesn t care. unaffiliated who su ...</td> <td>1      </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1070_Cleand_Climate1.csv</td> <td>RT @smilon713: I wish NO harm to those in red states. I  ...</td> <td>1      </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>80_Cleand_Climate1.csv  </td> <td>@LynchpinL With climate change making the world essentia ...</td> <td>1      </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1121_Cleand_Climate1.csv</td> <td>RT @yeson1631: .@VanJones68 all the life we know is deep ...</td> <td>1      </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (1400 rows omitted)</p>"
      ],
      "text/plain": [
       "ID                       | Text                                                         | Support\n",
       "1075_Cleand_Climate1.csv | RT @sydneyleemarco: nothing like an 80 degree october da ... | 1\n",
       "85_Cleand_Climate1.csv   | @MerlenesMemos @CNN It's not an act of god. Climate chan ... | 1\n",
       "916_Cleand_Climate1.csv  | RT @gq_jayq: Bet I got 11 years to run it up https://t.c ... | 1\n",
       "557_Cleand_Climate1.csv  | RT @HazelMonforton: Literally none of these things will  ... | 1\n",
       "39_Cleand_Climate1.csv   | @BjornLomborg I think looking at climate change in terms ... | 1\n",
       "608_Cleand_Climate1.csv  | RT @KateRaworth: If you feel the dissonance between the  ... | 1\n",
       "121_Cleand_Climate1.csv  | @Weinsteinlaw His base doesn t care. unaffiliated who su ... | 1\n",
       "1070_Cleand_Climate1.csv | RT @smilon713: I wish NO harm to those in red states. I  ... | 1\n",
       "80_Cleand_Climate1.csv   | @LynchpinL With climate change making the world essentia ... | 1\n",
       "1121_Cleand_Climate1.csv | RT @yeson1631: .@VanJones68 all the life we know is deep ... | 1\n",
       "... (1400 rows omitted)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3_positive = t3.where('Support', are.equal_to(1))\n",
    "t3_negative = t3.where('Support', are.equal_to(0))\n",
    "t3_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = t3_positive.append(t3_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged data input size 2883\n",
      "tagged data target prediction size 2883\n"
     ]
    }
   ],
   "source": [
    "data_tagged_X = list(data['Text'])\n",
    "data_tagged_Y = list(data['Support'])\n",
    "print('tagged data input size', len(data_tagged_X))\n",
    "print('tagged data target prediction size', len(data_tagged_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=10,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=[('a', 'a'...       min_weight_fraction_leaf=0.0, presort=False, random_state=100,\n",
       "            splitter='best'))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=Pipeline([('vect',CountVectorizer(stop_words=[(\"a\", \"a\"), (\"an\", \"n\"), (\"the\", \"e\"), (\"this\", \"s\"), (\"that\", \"t\"), (\"is\", \"s\")], token_pattern='(([#@]|[0-9]|[a-z]|[A-Z])+)',\n",
    "                                            analyzer='word',min_df = 10 )),\n",
    "                   ('clf',tree.DecisionTreeClassifier(criterion='entropy',random_state=100,max_depth=7,\n",
    "                      min_samples_leaf=10))])\n",
    "clf.fit(data_tagged_X,data_tagged_Y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold Number: 1\n",
      "Training Data Index: [   0    1    2 ... 2879 2881 2882]\n",
      "Testing Data Index: [   3   12   19   37   47   48   49   51   56   60   65   73   75   80\n",
      "   81   87   88   91   94   98  101  107  108  111  115  119  120  131\n",
      "  133  135  142  159  163  167  169  177  181  186  190  194  195  198\n",
      "  201  204  215  223  231  236  248  255  258  259  267  268  270  280\n",
      "  283  288  292  298  301  302  304  309  311  321  325  330  335  341\n",
      "  348  350  368  372  375  383  386  390  399  401  403  404  408  409\n",
      "  418  419  422  439  442  443  453  460  464  471  480  484  487  490\n",
      "  494  496  498  503  509  512  527  529  531  536  537  541  547  549\n",
      "  556  561  571  572  573  575  577  581  589  613  628  631  634  636\n",
      "  639  644  647  653  670  672  675  680  682  701  702  703  705  713\n",
      "  726  730  735  736  737  757  764  765  766  778  779  780  782  785\n",
      "  786  791  797  800  812  817  823  825  826  828  834  857  859  861\n",
      "  862  864  875  881  888  892  895  898  909  920  926  931  935  936\n",
      "  940  953  957  961  966  969  972  976  991  992 1006 1009 1011 1012\n",
      " 1027 1029 1030 1033 1038 1040 1044 1053 1054 1060 1064 1069 1073 1074\n",
      " 1076 1079 1081 1094 1098 1102 1104 1109 1115 1116 1119 1131 1135 1138\n",
      " 1142 1144 1152 1165 1167 1172 1174 1176 1177 1178 1182 1183 1184 1189\n",
      " 1197 1200 1201 1207 1210 1211 1221 1223 1224 1227 1230 1234 1241 1243\n",
      " 1244 1249 1253 1258 1273 1279 1282 1284 1292 1295 1297 1298 1317 1320\n",
      " 1325 1329 1335 1341 1345 1350 1355 1366 1368 1370 1371 1376 1388 1390\n",
      " 1403 1407 1413 1422 1429 1440 1447 1458 1459 1461 1463 1466 1470 1475\n",
      " 1483 1485 1490 1491 1497 1498 1501 1504 1511 1514 1517 1518 1521 1525\n",
      " 1529 1530 1543 1550 1552 1568 1569 1577 1579 1587 1591 1596 1600 1604\n",
      " 1608 1611 1614 1633 1637 1641 1646 1658 1665 1668 1669 1674 1677 1678\n",
      " 1680 1693 1698 1702 1709 1711 1712 1714 1719 1721 1724 1731 1735 1740\n",
      " 1751 1778 1785 1792 1796 1800 1809 1810 1813 1814 1818 1819 1824 1828\n",
      " 1832 1834 1838 1842 1852 1853 1856 1870 1874 1888 1890 1891 1897 1900\n",
      " 1905 1906 1908 1913 1930 1931 1935 1937 1941 1944 1957 1963 1966 1969\n",
      " 1971 1981 1985 1987 1989 1991 1999 2002 2003 2006 2008 2012 2017 2026\n",
      " 2035 2038 2039 2041 2046 2050 2053 2054 2063 2064 2069 2079 2081 2092\n",
      " 2093 2102 2104 2111 2112 2115 2121 2123 2129 2134 2139 2140 2145 2146\n",
      " 2152 2156 2158 2160 2175 2177 2178 2196 2203 2209 2211 2214 2218 2220\n",
      " 2223 2227 2229 2234 2237 2241 2244 2249 2256 2263 2265 2267 2270 2273\n",
      " 2278 2282 2287 2300 2302 2305 2309 2310 2312 2314 2316 2330 2334 2335\n",
      " 2337 2345 2346 2353 2358 2363 2367 2372 2388 2389 2394 2401 2402 2404\n",
      " 2410 2420 2422 2426 2427 2431 2434 2437 2473 2474 2478 2480 2484 2488\n",
      " 2489 2490 2493 2494 2495 2513 2516 2525 2527 2529 2535 2536 2537 2539\n",
      " 2541 2548 2551 2552 2555 2556 2559 2566 2567 2577 2588 2590 2598 2604\n",
      " 2605 2610 2613 2614 2618 2623 2625 2626 2629 2635 2637 2639 2643 2644\n",
      " 2650 2651 2654 2659 2663 2669 2675 2683 2692 2696 2697 2723 2737 2746\n",
      " 2757 2761 2778 2784 2792 2807 2839 2840 2842 2843 2847 2854 2860 2862\n",
      " 2873 2877 2880]\n",
      "Test data: Saving Decision Tree Visualization in  StanceClassifier-Fold1\n",
      "[[183 112]\n",
      " [137 145]]\n",
      "Precision Score: 0.5642023346303502\n",
      "Recall score: 0.5141843971631206\n",
      "f1 Score: 0.5380333951762523\n",
      "Fold Number: 2\n",
      "Training Data Index: [   0    1    2 ... 2880 2881 2882]\n",
      "Testing Data Index: [   6    8   10   13   17   27   34   35   41   53   58   59   62   76\n",
      "   78   83   85   90   99  104  125  126  134  139  140  148  154  156\n",
      "  165  175  180  188  189  191  200  202  205  207  208  211  216  220\n",
      "  226  228  234  237  239  241  242  244  250  260  262  264  274  275\n",
      "  286  299  303  306  310  314  315  320  323  326  329  331  336  340\n",
      "  345  347  349  351  355  363  370  382  393  395  400  407  414  424\n",
      "  426  428  430  432  435  446  452  457  459  463  473  478  481  482\n",
      "  491  493  495  516  520  521  525  528  534  535  538  546  553  558\n",
      "  559  579  584  587  593  596  597  598  599  607  612  614  616  620\n",
      "  622  625  632  638  640  641  643  654  659  660  661  663  664  669\n",
      "  671  674  683  685  692  693  694  697  698  711  719  720  724  725\n",
      "  729  731  742  746  752  755  761  763  767  768  772  776  777  793\n",
      "  799  801  808  809  813  816  819  820  830  841  852  853  855  858\n",
      "  860  867  880  899  902  913  918  924  927  930  943  945  947  951\n",
      "  962  963  977  979  986  989  997 1000 1003 1007 1008 1014 1020 1022\n",
      " 1023 1032 1051 1057 1058 1063 1066 1078 1082 1083 1085 1086 1091 1092\n",
      " 1093 1106 1108 1112 1113 1126 1129 1130 1133 1136 1139 1141 1143 1146\n",
      " 1147 1148 1151 1157 1162 1168 1171 1194 1198 1199 1203 1216 1218 1225\n",
      " 1229 1233 1237 1247 1252 1257 1265 1271 1285 1286 1289 1293 1304 1308\n",
      " 1309 1314 1319 1327 1330 1331 1339 1343 1344 1359 1361 1369 1373 1374\n",
      " 1382 1385 1416 1418 1419 1420 1427 1437 1444 1445 1451 1457 1468 1472\n",
      " 1477 1486 1488 1493 1495 1500 1508 1509 1535 1536 1541 1544 1545 1549\n",
      " 1558 1564 1566 1573 1575 1590 1598 1599 1601 1605 1610 1612 1615 1617\n",
      " 1618 1621 1625 1626 1630 1636 1638 1644 1647 1649 1651 1652 1654 1660\n",
      " 1670 1672 1684 1685 1690 1694 1696 1708 1713 1716 1720 1725 1730 1733\n",
      " 1739 1741 1745 1746 1757 1758 1759 1760 1761 1765 1773 1780 1782 1793\n",
      " 1802 1803 1805 1811 1817 1829 1831 1836 1840 1845 1849 1862 1863 1872\n",
      " 1881 1883 1892 1894 1901 1903 1904 1919 1922 1926 1938 1939 1945 1946\n",
      " 1947 1948 1951 1956 1968 1982 1983 1994 1997 2007 2009 2022 2023 2024\n",
      " 2032 2033 2042 2044 2045 2048 2049 2057 2059 2060 2070 2071 2073 2074\n",
      " 2084 2110 2113 2128 2135 2141 2155 2164 2165 2167 2169 2171 2173 2174\n",
      " 2176 2184 2186 2188 2189 2190 2195 2199 2200 2217 2233 2235 2238 2240\n",
      " 2243 2255 2262 2269 2274 2276 2290 2291 2295 2301 2303 2319 2323 2326\n",
      " 2329 2339 2340 2341 2347 2350 2351 2352 2355 2362 2365 2371 2380 2395\n",
      " 2396 2406 2414 2417 2421 2425 2430 2438 2439 2440 2445 2446 2450 2454\n",
      " 2457 2458 2461 2463 2469 2483 2486 2497 2500 2502 2507 2522 2526 2531\n",
      " 2534 2538 2542 2543 2568 2572 2573 2575 2586 2589 2592 2595 2596 2599\n",
      " 2602 2607 2609 2616 2628 2630 2636 2656 2661 2672 2673 2674 2676 2678\n",
      " 2680 2684 2690 2693 2700 2702 2704 2706 2707 2708 2717 2721 2727 2728\n",
      " 2729 2738 2748 2749 2750 2753 2759 2767 2768 2773 2779 2781 2785 2793\n",
      " 2796 2797 2808 2809 2810 2811 2812 2814 2816 2820 2831 2841 2845 2851\n",
      " 2855 2868 2871]\n",
      "Test data: Saving Decision Tree Visualization in  StanceClassifier-Fold2\n",
      "[[205  90]\n",
      " [133 149]]\n",
      "Precision Score: 0.6234309623430963\n",
      "Recall score: 0.5283687943262412\n",
      "f1 Score: 0.5719769673704415\n",
      "Fold Number: 3\n",
      "Training Data Index: [   3    4    6 ... 2880 2881 2882]\n",
      "Testing Data Index: [   0    1    2    5   11   16   23   28   31   32   33   40   45   46\n",
      "   50   54   57   61   64   66   68   69   72   74   84   89   92   97\n",
      "  103  106  110  113  114  117  118  121  127  132  136  137  146  151\n",
      "  160  161  164  172  178  185  187  192  193  218  224  233  238  245\n",
      "  246  247  251  254  256  257  265  276  277  281  282  289  291  293\n",
      "  294  295  297  307  308  318  322  332  339  354  358  364  366  367\n",
      "  374  381  385  387  388  389  411  415  425  429  433  434  436  437\n",
      "  441  445  455  472  474  479  483  486  492  502  507  530  548  550\n",
      "  552  554  555  560  567  568  570  576  578  582  585  588  594  602\n",
      "  604  619  623  629  646  649  650  651  655  665  673  678  679  686\n",
      "  699  700  707  708  709  710  718  741  744  745  750  754  759  769\n",
      "  774  775  789  790  798  804  807  810  815  821  824  827  831  833\n",
      "  839  840  851  866  868  872  876  877  885  887  890  891  893  894\n",
      "  897  900  906  912  916  921  925  929  937  938  941  970  978  983\n",
      "  984  985  990  993  994  995 1005 1013 1016 1017 1021 1026 1028 1035\n",
      " 1047 1048 1065 1070 1071 1084 1087 1089 1090 1100 1103 1105 1114 1118\n",
      " 1121 1122 1123 1124 1127 1132 1149 1153 1154 1156 1163 1166 1169 1185\n",
      " 1204 1213 1215 1219 1231 1235 1245 1248 1250 1259 1260 1263 1264 1269\n",
      " 1275 1280 1283 1290 1294 1299 1311 1313 1315 1322 1324 1334 1336 1338\n",
      " 1342 1346 1352 1353 1364 1372 1375 1378 1380 1383 1386 1392 1396 1399\n",
      " 1400 1401 1410 1411 1412 1415 1421 1423 1426 1433 1438 1441 1442 1443\n",
      " 1448 1450 1455 1456 1460 1464 1467 1469 1471 1474 1476 1478 1479 1482\n",
      " 1484 1494 1507 1513 1516 1520 1523 1527 1531 1537 1546 1547 1561 1570\n",
      " 1571 1574 1582 1585 1595 1597 1603 1628 1634 1643 1648 1655 1657 1661\n",
      " 1666 1667 1675 1686 1687 1691 1692 1701 1703 1704 1705 1707 1710 1717\n",
      " 1718 1728 1732 1736 1742 1749 1750 1755 1764 1768 1774 1776 1777 1784\n",
      " 1795 1797 1798 1821 1825 1835 1837 1843 1844 1847 1851 1855 1858 1865\n",
      " 1867 1869 1873 1882 1884 1889 1893 1896 1902 1912 1917 1920 1940 1958\n",
      " 1959 1960 1964 1965 1970 1977 1978 1986 1988 1992 1995 2004 2014 2029\n",
      " 2030 2051 2056 2065 2075 2080 2082 2083 2085 2088 2089 2090 2095 2096\n",
      " 2103 2107 2108 2109 2117 2118 2119 2120 2130 2136 2147 2151 2154 2162\n",
      " 2179 2181 2182 2183 2187 2192 2201 2207 2208 2210 2219 2222 2225 2226\n",
      " 2230 2231 2236 2250 2251 2252 2261 2268 2271 2277 2280 2286 2298 2304\n",
      " 2308 2331 2332 2336 2360 2361 2369 2376 2379 2382 2387 2390 2392 2393\n",
      " 2400 2403 2405 2407 2413 2415 2418 2419 2423 2424 2432 2433 2436 2447\n",
      " 2448 2455 2464 2465 2467 2468 2472 2476 2479 2492 2499 2503 2504 2510\n",
      " 2512 2514 2518 2519 2528 2533 2540 2544 2545 2553 2557 2562 2563 2564\n",
      " 2569 2578 2580 2581 2582 2583 2608 2611 2615 2617 2621 2633 2638 2648\n",
      " 2655 2660 2662 2666 2677 2679 2681 2686 2705 2711 2720 2724 2733 2735\n",
      " 2739 2743 2744 2752 2756 2758 2762 2763 2764 2772 2775 2777 2789 2800\n",
      " 2803 2805 2806 2813 2818 2819 2834 2835 2836 2837 2846 2849 2864 2867\n",
      " 2869 2872 2874]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data: Saving Decision Tree Visualization in  StanceClassifier-Fold3\n",
      "[[208  87]\n",
      " [136 146]]\n",
      "Precision Score: 0.6266094420600858\n",
      "Recall score: 0.5177304964539007\n",
      "f1 Score: 0.566990291262136\n",
      "Fold Number: 4\n",
      "Training Data Index: [   0    1    2 ... 2880 2881 2882]\n",
      "Testing Data Index: [   4    7    9   14   18   22   26   29   30   36   38   39   42   52\n",
      "   67   71   82   93   95  102  105  112  116  122  124  128  147  153\n",
      "  157  158  162  168  173  179  184  197  203  212  213  214  217  221\n",
      "  225  227  232  249  261  271  272  273  284  285  287  296  300  305\n",
      "  312  328  333  334  337  342  343  344  346  352  353  356  359  360\n",
      "  361  362  365  371  373  377  378  379  380  392  394  396  397  402\n",
      "  406  412  421  427  447  448  449  454  462  466  467  476  488  499\n",
      "  500  501  504  506  510  511  514  517  518  519  522  523  533  540\n",
      "  543  544  551  557  563  565  574  590  592  600  603  605  608  611\n",
      "  618  635  645  658  662  667  676  677  684  688  691  704  716  721\n",
      "  723  733  734  738  743  747  748  756  758  762  770  771  773  781\n",
      "  783  787  794  795  796  802  803  805  814  818  822  835  836  838\n",
      "  842  844  845  846  848  850  854  856  863  870  873  874  886  903\n",
      "  904  908  915  917  919  922  923  932  934  939  942  944  946  948\n",
      "  950  952  955  956  958  959  968  971  980  982  987  988  996  998\n",
      " 1004 1010 1015 1018 1024 1036 1037 1041 1042 1043 1045 1055 1059 1062\n",
      " 1068 1080 1097 1101 1107 1117 1125 1128 1134 1140 1145 1158 1159 1170\n",
      " 1173 1179 1181 1186 1188 1192 1196 1205 1206 1208 1209 1212 1228 1232\n",
      " 1238 1242 1246 1254 1256 1261 1262 1267 1268 1274 1276 1296 1307 1323\n",
      " 1328 1332 1333 1340 1348 1349 1358 1379 1384 1391 1393 1394 1397 1402\n",
      " 1404 1406 1414 1417 1424 1428 1432 1436 1439 1446 1449 1452 1462 1481\n",
      " 1492 1499 1502 1503 1505 1512 1515 1522 1524 1526 1528 1532 1534 1538\n",
      " 1542 1556 1557 1563 1567 1572 1578 1583 1588 1589 1594 1602 1607 1613\n",
      " 1622 1623 1624 1627 1631 1635 1640 1642 1656 1659 1664 1671 1681 1682\n",
      " 1683 1695 1697 1699 1706 1715 1722 1738 1743 1744 1747 1752 1753 1754\n",
      " 1756 1762 1763 1766 1769 1770 1771 1772 1775 1781 1783 1787 1788 1789\n",
      " 1790 1791 1799 1804 1806 1807 1812 1816 1822 1839 1846 1857 1859 1864\n",
      " 1876 1877 1886 1898 1909 1910 1911 1914 1916 1921 1924 1927 1928 1929\n",
      " 1932 1933 1943 1950 1953 1954 1961 1962 1967 1973 1975 1980 1984 1998\n",
      " 2000 2010 2013 2015 2018 2021 2028 2055 2061 2062 2068 2072 2077 2086\n",
      " 2087 2094 2098 2101 2114 2126 2131 2133 2143 2144 2148 2153 2157 2166\n",
      " 2168 2172 2180 2185 2191 2193 2197 2204 2205 2206 2212 2213 2215 2224\n",
      " 2228 2232 2245 2246 2248 2254 2258 2260 2264 2266 2272 2283 2284 2285\n",
      " 2296 2297 2307 2313 2318 2322 2325 2327 2328 2333 2342 2344 2348 2349\n",
      " 2354 2356 2357 2366 2368 2373 2378 2381 2386 2397 2398 2399 2408 2416\n",
      " 2428 2442 2443 2451 2452 2453 2470 2475 2481 2491 2496 2501 2508 2511\n",
      " 2515 2517 2523 2524 2532 2546 2549 2550 2554 2558 2576 2579 2587 2591\n",
      " 2593 2594 2606 2619 2620 2631 2634 2640 2642 2647 2652 2653 2658 2664\n",
      " 2667 2670 2671 2685 2691 2694 2695 2699 2701 2709 2712 2718 2719 2725\n",
      " 2726 2732 2740 2741 2754 2760 2771 2776 2787 2788 2791 2794 2798 2817\n",
      " 2822 2825 2829 2844 2850 2852 2856 2857 2858 2863 2865 2866 2870 2875\n",
      " 2876 2879]\n",
      "Test data: Saving Decision Tree Visualization in  StanceClassifier-Fold4\n",
      "[[197  97]\n",
      " [121 161]]\n",
      "Precision Score: 0.624031007751938\n",
      "Recall score: 0.5709219858156028\n",
      "f1 Score: 0.5962962962962962\n",
      "Fold Number: 5\n",
      "Training Data Index: [   0    1    2 ... 2877 2879 2880]\n",
      "Testing Data Index: [  15   20   21   24   25   43   44   55   63   70   77   79   86   96\n",
      "  100  109  123  129  130  138  141  143  144  145  149  150  152  155\n",
      "  166  170  171  174  176  182  183  196  199  206  209  210  219  222\n",
      "  229  230  235  240  243  252  253  263  266  269  278  279  290  313\n",
      "  316  317  319  324  327  338  357  369  376  384  391  398  405  410\n",
      "  413  416  417  420  423  431  438  440  444  450  451  456  458  461\n",
      "  465  468  469  470  475  477  485  489  497  505  508  513  515  524\n",
      "  526  532  539  542  545  562  564  566  569  580  583  586  591  595\n",
      "  601  606  609  610  615  617  621  624  626  627  630  633  637  642\n",
      "  648  652  656  657  666  668  681  687  689  690  695  696  706  712\n",
      "  714  715  717  722  727  728  732  739  740  749  751  753  760  784\n",
      "  788  792  806  811  829  832  837  843  847  849  865  869  871  878\n",
      "  879  882  883  884  889  896  901  905  907  910  911  914  928  933\n",
      "  949  954  960  964  965  967  973  974  975  981  999 1001 1002 1019\n",
      " 1025 1031 1034 1039 1046 1049 1050 1052 1056 1061 1067 1072 1075 1077\n",
      " 1088 1095 1096 1099 1110 1111 1120 1137 1150 1155 1160 1161 1164 1175\n",
      " 1180 1187 1190 1191 1193 1195 1202 1214 1217 1220 1222 1226 1236 1239\n",
      " 1240 1251 1255 1266 1270 1272 1277 1278 1281 1287 1288 1291 1300 1301\n",
      " 1302 1303 1305 1306 1310 1312 1316 1318 1321 1326 1337 1347 1351 1354\n",
      " 1356 1357 1360 1362 1363 1365 1367 1377 1381 1387 1389 1395 1398 1405\n",
      " 1408 1409 1425 1430 1431 1434 1435 1453 1454 1465 1473 1480 1487 1489\n",
      " 1496 1506 1510 1519 1533 1539 1540 1548 1551 1553 1554 1555 1559 1560\n",
      " 1562 1565 1576 1580 1581 1584 1586 1592 1593 1606 1609 1616 1619 1620\n",
      " 1629 1632 1639 1645 1650 1653 1662 1663 1673 1676 1679 1688 1689 1700\n",
      " 1723 1726 1727 1729 1734 1737 1748 1767 1779 1786 1794 1801 1808 1815\n",
      " 1820 1823 1826 1827 1830 1833 1841 1848 1850 1854 1860 1861 1866 1868\n",
      " 1871 1875 1878 1879 1880 1885 1887 1895 1899 1907 1915 1918 1923 1925\n",
      " 1934 1936 1942 1949 1952 1955 1972 1974 1976 1979 1990 1993 1996 2001\n",
      " 2005 2011 2016 2019 2020 2025 2027 2031 2034 2036 2037 2040 2043 2047\n",
      " 2052 2058 2066 2067 2076 2078 2091 2097 2099 2100 2105 2106 2116 2122\n",
      " 2124 2125 2127 2132 2137 2138 2142 2149 2150 2159 2161 2163 2170 2194\n",
      " 2198 2202 2216 2221 2239 2242 2247 2253 2257 2259 2275 2279 2281 2288\n",
      " 2289 2292 2293 2294 2299 2306 2311 2315 2317 2320 2321 2324 2338 2343\n",
      " 2359 2364 2370 2374 2375 2377 2383 2384 2385 2391 2409 2411 2412 2429\n",
      " 2435 2441 2444 2449 2456 2459 2460 2462 2466 2471 2477 2482 2485 2487\n",
      " 2498 2505 2506 2509 2520 2521 2530 2547 2560 2561 2565 2570 2571 2574\n",
      " 2584 2585 2597 2600 2601 2603 2612 2622 2624 2627 2632 2641 2645 2646\n",
      " 2649 2657 2665 2668 2682 2687 2688 2689 2698 2703 2710 2713 2714 2715\n",
      " 2716 2722 2730 2731 2734 2736 2742 2745 2747 2751 2755 2765 2766 2769\n",
      " 2770 2774 2780 2782 2783 2786 2790 2795 2799 2801 2802 2804 2815 2821\n",
      " 2823 2824 2826 2827 2828 2830 2832 2833 2838 2848 2853 2859 2861 2878\n",
      " 2881 2882]\n",
      "Test data: Saving Decision Tree Visualization in  StanceClassifier-Fold5\n",
      "[[220  74]\n",
      " [124 158]]\n",
      "Precision Score: 0.6810344827586207\n",
      "Recall score: 0.5602836879432624\n",
      "f1 Score: 0.6147859922178989\n"
     ]
    }
   ],
   "source": [
    "fold = 1\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "for train_index, test_index in skf.split(data_tagged_X, data_tagged_Y):\n",
    "    print(\"Fold Number:\", fold)\n",
    "    print(\"Training Data Index:\", train_index)\n",
    "    print(\"Testing Data Index:\", test_index)\n",
    "    \n",
    "    x_train = list(data.take(train_index)['Text'])\n",
    "    y_train = list(data.take(train_index)['Support'])\n",
    "    x_test = list(data.take(test_index)['Text'])\n",
    "    y_test = list(data.take(test_index)['Support'])\n",
    "    \n",
    "    count_vect = CountVectorizer(stop_words=[(\"a\", \"a\"), (\"an\", \"n\"), (\"the\", \"e\"), (\"this\", \"s\"), (\"that\", \"t\"), (\"is\", \"s\")],\n",
    "                                              token_pattern='(([#@]|[0-9]|[a-z]|[A-Z])+)',analyzer='word',min_df= 10)\n",
    "    X_word_vect = count_vect.fit_transform(x_train)\n",
    "    clf = tree.DecisionTreeClassifier(criterion='entropy', random_state = 100, max_depth=7, min_samples_leaf = 10)\n",
    "    clf.fit(X_word_vect, y_train) \n",
    "    dot_data = tree.export_graphviz(clf, out_file = None, feature_names = count_vect.get_feature_names())\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    file_name = 'StanceClassifier-Fold' + str(fold)\n",
    "    graph.render(file_name)\n",
    "    print(\"Test data: Saving Decision Tree Visualization in \",file_name)\n",
    "   \n",
    "    x_test_word_vect= count_vect.transform(x_test)\n",
    "    predicted_y_test = clf.predict(x_test_word_vect) \n",
    "    #print(\\\"Testing Data Prediciton Output:\\\", predicted_y_test)\n",
    "    print(metrics.confusion_matrix(y_test, predicted_y_test))\n",
    "   \n",
    "    precision.append(precision_score(y_test, predicted_y_test))\n",
    "    recall.append(recall_score(y_test, predicted_y_test))\n",
    "    f1.append(f1_score(y_test,predicted_y_test)) \n",
    "    print(\"Precision Score:\", precision_score(y_test, predicted_y_test))\n",
    "    print(\"Recall score:\", recall_score(y_test, predicted_y_test))\n",
    "    print(\"f1 Score:\", f1_score(y_test, predicted_y_test))\n",
    "    fold = fold+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision List: [0.5642023346303502, 0.6234309623430963, 0.6266094420600858, 0.624031007751938, 0.6810344827586207]\n",
      "Recall List [0.5141843971631206, 0.5283687943262412, 0.5177304964539007, 0.5709219858156028, 0.5602836879432624]\n",
      "f1 List: [0.5380333951762523, 0.5719769673704415, 0.566990291262136, 0.5962962962962962, 0.6147859922178989]\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision List:\", precision)\n",
    "print(\"Recall List\", recall)\n",
    "print(\"f1 List:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 0.6238616459088182\n",
      "Average Recall: 0.5382978723404255\n",
      "Average F1: 0.577616588464605\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Precision:\", np.mean(precision))\n",
    "print(\"Average Recall:\", np.mean(recall))\n",
    "print(\"Average F1:\", np.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_entire_X = list(data['Text'])\n",
    "data_entire_Y = list(data['Support'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=10,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=[('a', 'a'...       min_weight_fraction_leaf=0.0, presort=False, random_state=100,\n",
       "            splitter='best'))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=Pipeline([('vect',CountVectorizer(stop_words=[(\"a\", \"a\"), (\"an\", \"n\"), (\"the\", \"e\"), (\"this\", \"s\"), (\"that\", \"t\"), (\"is\", \"s\")], token_pattern='(([#@]|[0-9]|[a-z]|[A-Z])+)',\n",
    "                                            analyzer='word',min_df = 10 )),\n",
    "                   ('clf',tree.DecisionTreeClassifier(criterion='entropy',random_state=100,max_depth=7,\n",
    "                      min_samples_leaf=10))])\n",
    "clf.fit(data_entire_X,data_entire_Y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ClimateTeam20PD2A.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, 'ClimateTeam20PD2A.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = joblib.load('ClimateTeam20PD2A.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_Y = clf2.predict(data_entire_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 Score of labelled data: 0.6301067353698933\n"
     ]
    }
   ],
   "source": [
    " print(\"f1 Score of labelled data:\", f1_score(data_tagged_Y, predicted_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
